{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from tavily import TavilyClient\n",
    "import os\n",
    "hf_token = \"\"\n",
    "login(hf_token)\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "CONFIG = {\n",
    "    \"model_name\": \"Alibaba-NLP/gte-large-en-v1.5\",\n",
    "    \"collection_name\": \"vulnerabilities\",\n",
    "    \"persistent_dir\": \"./chromadb_part2\",\n",
    "}\n",
    "\n",
    "def load_vectorstore():\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=CONFIG[\"model_name\"],\n",
    "        model_kwargs={\"trust_remote_code\": True}\n",
    "    )\n",
    "    \n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=CONFIG[\"persistent_dir\"],\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=CONFIG[\"collection_name\"]\n",
    "    )\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "def query_vectorstore(vectorstore, query: str, k: int):\n",
    "    try:\n",
    "        results = vectorstore.similarity_search(query, k=k)\n",
    "        print(\"\\nQuery Results:\")\n",
    "        for i, doc in enumerate(results, 1):\n",
    "            print(f\"\\nResult {i}:\")\n",
    "            print(f\"Content: {doc.page_content[:200]}...\")  # Show first 200 characters\n",
    "            print(f\"Metadata: {doc.metadata}\")\n",
    "            print(\"-\" * 80)  # Separator between results\n",
    "    except Exception as e: \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = load_vectorstore()\n",
    "retriever = vectorstore.as_retriever(k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_unique_test_case_numbers(test_cases):\n",
    "    for index, tc in enumerate(test_cases, start=1):\n",
    "        tc.name = f\"Test Case {index}: {tc.name.split(':', 1)[-1].strip()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_test_cases(test_cases):\n",
    "    seen_ids = set()\n",
    "    verified_test_cases = []\n",
    "    for tc in test_cases:\n",
    "        if tc.id not in seen_ids:\n",
    "            seen_ids.add(tc.id)\n",
    "            verified_test_cases.append(tc)\n",
    "        else:\n",
    "            print(f\"Duplicate test case id found: {tc.id}\")\n",
    "    return verified_test_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class DocumentRetrievalPrompt(BaseModel):\n",
    "    keywords: List[str] = Field(description=\"List of keywords extracted from the attack tree analysis\")\n",
    "    vulnerabilities: List[dict] = Field(description=\"List of identified vulnerabilities with details\")\n",
    "    query: str = Field(description=\"A comprehensive query for document retrieval\")\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=DocumentRetrievalPrompt)\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1:70b\", temperature=0)\n",
    "\n",
    "vulnearbility_prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a senior security engineer tasked with performing an in-depth analysis of an attack tree to identify all possible vulnerabilities in the system. Your analysis should be thorough and comprehensive, leaving no stone unturned.\n",
    "\n",
    "Here is the attack tree in JSON format:\n",
    "\n",
    "{attack_tree}\n",
    "\n",
    "Please perform the following tasks:\n",
    "1. Conduct a detailed, step-by-step analysis of the attack tree, considering all possible attack vectors and their implications.\n",
    "2. Identify and Describe all potential vulnerabilities in the system, including their severity, potential impact, and possible mitigation strategies.\n",
    "3. Extract relevant keywords from the attack tree that are crucial for understanding the system's security landscape.\n",
    "4. Create a comprehensive query for document retrieval that covers all aspects of the attack tree and identified vulnerabilities.\n",
    "\n",
    "Ensure your analysis is exhaustive and doesn't overlook any potential security risks. Consider both obvious and non-obvious attack paths.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Provide your in-depth analysis and the document retrieval prompt in the specified JSON format.\"\"\",\n",
    "    input_variables=[\"attack_tree\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "\n",
    "attack_tree_analyzer = vulnearbility_prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from typing import List\n",
    "\n",
    "class TestCase(BaseModel):\n",
    "    id: int = Field(description=\"Unique identifier for the test case\")\n",
    "    name: str = Field(description=\"Name of the test case\")\n",
    "    description: str = Field(description=\"Detailed description of what the test case is checking\")\n",
    "    vulnerability_addressed: str = Field(description=\"The specific vulnerability this test case is addressing\")\n",
    "    setup: str = Field(description=\"Setup code for the test case\")\n",
    "    test_code: str = Field(description=\"Actual test code\")\n",
    "    teardown: str = Field(description=\"Teardown code for the test case\")\n",
    "    expected_result: str = Field(description=\"Expected result of the test\")\n",
    "\n",
    "class TestCaseSet(BaseModel):\n",
    "    test_cases: List[TestCase] = Field(description=\"List of generated detailed test cases\")\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=TestCaseSet)\n",
    "\n",
    "test_case_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an elite security test engineer with extensive experience in creating comprehensive, robust, and detailed Python test suites, specifically targeting vulnerabilities in diverse systems and infrastructures. Your task is to generate an exceptionally thorough set of Python test cases for a system based on a detailed attack tree analysis and additional context documents. The goal is to create a test suite that rigorously validates the security of the system and defends against ALL identified vulnerabilities, no matter how complex or nuanced.\n",
    "\n",
    "**Attack Tree**:\n",
    "{attack_tree}\n",
    "\n",
    "**Analysed Vulnerabilities**:\n",
    "{attack_tree_analysis}\n",
    "\n",
    "**Additional Context Documents**:\n",
    "{context_documents}\n",
    "\n",
    "### Critical Requirements for the Test Suite:\n",
    "1. **Vulnerability Focus**: Each test case must be designed to specifically test for vulnerabilities mentioned in the attack tree, focusing on real security issues like improper access control, misconfigurations, insecure data storage, unencrypted data transmission, excessive permissions, and API abuse. Go beyond availability checks to test for security weaknesses.\n",
    "2. **Alignment with Attack Tree**: Create a separate test case for each attack vector identified in the attack tree analysis. No vulnerability should be left untested. Ensure test cases mirror the attack vectors, addressing both the specific scenarios and potential bypass methods.\n",
    "3. **Depth in Vulnerability Testing**: For complex vulnerabilities, create multiple test methods to cover various scenarios, edge cases, and potential bypass methods, ensuring thorough coverage. Incorporate multiple stages of attacks (e.g., privilege escalation, lateral movement, denial of service).\n",
    "4. **Realistic Attack Scenarios**: Use realistic, diverse, and complex data sets in your tests, simulating real-world usage and attack scenarios relevant to the system under test. The test cases should go beyond basic functionality checks to simulate attack patterns like privilege escalation, improper role assignment, and compromised credentials.\n",
    "5. **Security Assertions**: Implement sophisticated assertions that not only verify functionality but also confirm the absence of security vulnerabilities (e.g., improper access permissions, unencrypted data storage, misconfigured security settings).\n",
    "6. **System-Specific Vulnerability Testing**: Ensure test cases cover security aspects of the specific components or services of the system, focusing on vulnerability testing rather than just availability. For instance, test improper access control for data storage, unencrypted communication channels, insecure network configurations, etc.\n",
    "7. **Positive and Negative Tests**: Implement both positive tests (verifying secure configurations and expected behavior) and negative tests (attempting to exploit vulnerabilities) for each vulnerability.\n",
    "8. **Parameterized Tests**: Where applicable, implement parameterized tests to cover a wide range of inputs efficiently, ensuring scalability in vulnerability testing.\n",
    "9. **Race Conditions and Timing Attacks**: Include tests for complex vulnerabilities like race conditions, timing attacks, and multi-stage attacks that simulate real-world advanced persistent threat (APT) scenarios.\n",
    "10. **Error Handling**: Include error-handling tests to ensure that the system behaves securely under various error conditions, such as handling malformed requests or insufficient permissions securely.\n",
    "11. **Comprehensive Coverage**: Maintain comprehensive coverage of all system components but ensure that the tests are focused on testing security vulnerabilities (e.g., roles with excessive permissions, unencrypted data storage, misconfigured security alerts that fail to detect incidents).\n",
    "12. **Thorough Documentation**: Add exhaustive comments and docstrings explaining:\n",
    "    - The purpose of each test method\n",
    "    - The specific vulnerability or attack vector being tested\n",
    "    - The expected outcome and why it's secure\n",
    "    - Any subtle points or non-obvious security implications\n",
    "13. **Runnable Code**: Provide Python code for each test case that is immediately runnable. Ensure the use of the `unittest` framework and appropriate mocking techniques where applicable, but move beyond basic mocking to simulate real-world attack scenarios.\n",
    "14. **Coverage Report**: After generating the test cases, provide a coverage report explaining how each identified vulnerability is addressed by the test cases.\n",
    "\n",
    "### Example Format:\n",
    "Each test case should include:\n",
    "- **Setup**: Any necessary setup, such as configuring system components, setting user roles and permissions, or initializing data stores.\n",
    "- **Test Method**: The test logic that simulates the attack and checks for the presence of the vulnerability. The method should include positive tests (for secure configurations) and negative tests (for attempted exploits).\n",
    "- **Teardown**: Any necessary cleanup to ensure tests don't affect each other.\n",
    "- **Expected Result**: The expected behavior when the system is secure and the vulnerability is absent.\n",
    "                                                    \n",
    "Each test case should include an 'id' field that is a unique integer.\n",
    "                                                    \n",
    "{format_instructions}\n",
    "\n",
    "Remember to provide actual Python code for each test case, not just descriptions, and ensure ALL vulnerabilities are covered.\n",
    "\"\"\")\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1:70b\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "rag_testcase_generator = test_case_prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1:70b\", format=\"json\", temperature=0)\n",
    "\n",
    "\n",
    "grader_prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an expert security analyst grading the relevance of a retrieved document to vulnerabilities identified in an attack tree analysis.\n",
    "\n",
    "Analyzed Vulnerability:\n",
    "{vulnerability}\n",
    "\n",
    "Retrieved Document:\n",
    "{document}\n",
    "\n",
    "Grade the document's relevance to the vulnerability based on the following criteria:\n",
    "1. The document discusses the specific vulnerability or closely related security issues.\n",
    "2. The document provides relevant information for understanding or mitigating the vulnerability.\n",
    "3. The document contains example code or test cases that could be adapted to test for this vulnerability.\n",
    "\n",
    "Provide a binary score as a JSON with a single key 'score':\n",
    "- Use 'yes' if the document is relevant and meets at least two of the above criteria.\n",
    "- Use 'no' if the document is not relevant or meets fewer than two criteria.\n",
    "\n",
    "Return only the JSON object with no preamble or explanation.\"\"\",\n",
    "    input_variables=[\"vulnerability\", \"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = grader_prompt | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "# Define the output parser if not already defined\n",
    "output_parser = PydanticOutputParser(pydantic_object=TestCaseSet)\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1:70b\", format=\"json\", temperature=0)\n",
    "\n",
    "regenerate_test_case_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an elite security test engineer with extensive experience in creating comprehensive and robust test suites across various systems and infrastructures. Your critical task is to **modify the existing test cases** based on the improvement suggestions provided, and **add new test cases** for any missing vulnerabilities.\n",
    "\n",
    "### Existing Test Cases:\n",
    "{existing_test_cases}\n",
    "\n",
    "### Improvement Suggestions:\n",
    "{improvement_suggestions}\n",
    "\n",
    "### Missing Vulnerabilities:\n",
    "{missing_vulnerabilities}\n",
    "\n",
    "### Instructions:\n",
    "1. **Modify the existing test cases** to incorporate all improvement suggestions. Only make changes where improvements are suggested; retain other content.\n",
    "2. For each **missing vulnerability**, **create a new test case** that exactly addresses the vulnerability.\n",
    "3. Ensure that all test cases use appropriate and actual code relevant to the system under test, utilizing standard libraries or APIs suitable for that system.\n",
    "4. Include all necessary **setup**, including required imports and initialization of system components or services if needed.\n",
    "5. The test code must be **complete, runnable Python code**. Do not use pseudocode or placeholders.\n",
    "6. Follow **best practices** for the system or domain you are testing, and use appropriate methods and calls.\n",
    "7. Each test case should demonstrate both the **vulnerable state and the secure state**.\n",
    "8. Use **assert statements** to clearly indicate what constitutes a pass or fail condition.\n",
    "9. Each test case should include an 'id' field that is a unique integer.\n",
    "{format_instructions}\n",
    "\"\"\")\n",
    "\n",
    "regenerate_test_case_generator = regenerate_test_case_prompt | llm | output_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict, List\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langchain.vectorstores import VectorStore\n",
    "import json\n",
    "from langchain_core.messages import AIMessage\n",
    "from IPython.display import Image, display\n",
    "from langchain.schema import Document\n",
    "from requests.exceptions import HTTPError\n",
    "import re\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        attack_tree: JSON representation of the attack tree\n",
    "        analysis: Result of the vulnerability analysis\n",
    "        retrieval_prompt: Generated prompt for document retrieval\n",
    "        documents: List of retrieved documents\n",
    "        test_cases: Generated test cases\n",
    "        steps: List of steps taken\n",
    "        document_grades: List of document grades\n",
    "        search_needed: Flag to determine if web search is needed\n",
    "        alignment_check: Alignment check results\n",
    "        regeneration_attempts: Number of regeneration attempts\n",
    "    \"\"\"\n",
    "    attack_tree: str\n",
    "    analysis: dict\n",
    "    retrieval_prompt: str\n",
    "    documents: List[Document]\n",
    "    test_cases: List[dict]\n",
    "    steps: List[str]\n",
    "    document_grades: List[dict]\n",
    "    search_needed: bool\n",
    "    alignment_check: dict\n",
    "    regeneration_attempts: int\n",
    "    vulnerabilities: dict\n",
    "\n",
    "\n",
    "def analyze_attack_tree(state):\n",
    "    attack_tree = state[\"attack_tree\"]\n",
    "    result = attack_tree_analyzer.invoke({\"attack_tree\": attack_tree})\n",
    "    \n",
    "    steps = state[\"steps\"]\n",
    "    steps.append(\"analyze_attack_tree\")\n",
    "\n",
    "    # Ensure that vulnerabilities are correctly extracted\n",
    "    vulnerabilities = result.vulnerabilities if hasattr(result, 'vulnerabilities') else []\n",
    "    \n",
    "    # Update the state with the extracted information\n",
    "    state.update({\n",
    "        \"analysis\": {\n",
    "            \"keywords\": result.keywords,\n",
    "            \"vulnerabilities\": vulnerabilities,\n",
    "            \"query\": result.query\n",
    "        },\n",
    "        \"retrieval_prompt\": result.query,\n",
    "        \"steps\": steps,\n",
    "    })\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def retrieve_documents(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents based on the vulnerabilities.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updated state with retrieved documents\n",
    "    \"\"\"\n",
    "    vulnerabilities = state[\"analysis\"][\"vulnerabilities\"]\n",
    "    steps = state[\"steps\"]\n",
    "    steps.append(\"retrieve_documents\")\n",
    "    documents = []\n",
    "\n",
    "    for vulnerability in vulnerabilities:\n",
    "        print(vulnerability)\n",
    "        try:\n",
    "            query = f\"{vulnerability['name']}: {vulnerability['description']}\"\n",
    "        except:\n",
    "            query = f\"{vulnerability['name']}\"\n",
    "            \n",
    "        # print(f\"Retrieving documents for vulnerability: {vulnerability['name']}\")\n",
    "        # docs = vectorstore.similarity_search(query, k=5)  # Retrieve 3 documents per vulnerability\n",
    "        # print(f\"Retrieved {len(docs)} documents for {vulnerability['name']}\")\n",
    "        docs = retriever.get_relevant_documents(query)  \n",
    "        #print(f\"Retrieved {len(docs)} documents for {vulnerability['name']}\")\n",
    "        documents.extend(docs)\n",
    "\n",
    "    state[\"documents\"] = documents\n",
    "    state[\"steps\"] = steps\n",
    "    return state\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    vulnerabilities = state[\"analysis\"][\"vulnerabilities\"]\n",
    "    documents = state[\"documents\"]\n",
    "    steps = state[\"steps\"]\n",
    "    steps.append(\"grade_documents\")\n",
    "    filtered_docs = []\n",
    "    document_grades = []\n",
    "    search_needed = False\n",
    "\n",
    "    for vulnerability in vulnerabilities:\n",
    "        #print(f\"Grading documents for vulnerability: {vulnerability['name']}\")\n",
    "        relevant_docs = []\n",
    "        for doc in documents:\n",
    "            try:\n",
    "                score = retrieval_grader.invoke(\n",
    "                    {\"vulnerability\": json.dumps(vulnerability), \"document\": doc.page_content}\n",
    "                )\n",
    "                grade = score[\"score\"]\n",
    "                document_grades.append({\n",
    "                    \"vulnerability\": vulnerability['name'],\n",
    "                    \"document_content\": doc.page_content[:100] + \"...\",\n",
    "                    \"grade\": grade\n",
    "                })\n",
    "                if grade.lower() == 'yes':\n",
    "                    relevant_docs.append(doc)\n",
    "            except Exception as e:\n",
    "                print(f\"Error grading document: {e}\")\n",
    "\n",
    "        #print(f\"Found {len(relevant_docs)} relevant documents for {vulnerability['name']}\")\n",
    "        if len(relevant_docs) < 3:  # Adjust this threshold as needed\n",
    "            search_needed = True\n",
    "            #print(f\"Not enough relevant documents for {vulnerability['name']}. Web search may be needed.\")\n",
    "        \n",
    "        filtered_docs.extend(relevant_docs)\n",
    "\n",
    "    state[\"documents\"] = filtered_docs\n",
    "    state[\"search_needed\"] = search_needed\n",
    "    state[\"steps\"] = steps\n",
    "    state[\"document_grades\"] = document_grades\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    analysis = state[\"analysis\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "    steps = state[\"steps\"]\n",
    "    steps.append(\"web_search\")\n",
    "    document_grades = state.get(\"document_grades\", [])\n",
    "\n",
    "    for vulnerability in analysis[\"vulnerabilities\"]:\n",
    "        # Ensure 'name' and 'description' exist in the vulnerability dictionary\n",
    "        name = vulnerability.get('name', 'Unknown Vulnerability')\n",
    "        description = vulnerability.get('description', 'No description provided')\n",
    "\n",
    "        # Construct a search-friendly query string\n",
    "        query = f\"{name} - {description}\"\n",
    "        print(f\"Constructed Web Search Query: {query}\")  # Debugging\n",
    "\n",
    "        try:\n",
    "            # Invoke the web search tool (simulated or actual web search)\n",
    "            web_results = web_search_tool.invoke({\"query\": query})\n",
    "            new_docs = [Document(page_content=d[\"content\"], metadata={\"url\": d[\"url\"]}) for d in web_results]\n",
    "            documents.extend(new_docs)\n",
    "\n",
    "            # Grade new documents\n",
    "            for doc in new_docs:\n",
    "                score = retrieval_grader.invoke(\n",
    "                    {\"vulnerability\": json.dumps(vulnerability), \"document\": doc.page_content}\n",
    "                )\n",
    "                grade = score[\"score\"]\n",
    "                document_grades.append({\n",
    "                    \"vulnerability\": str(vulnerability),\n",
    "                    \"document_content\": doc.page_content[:100] + \"...\",\n",
    "                    \"grade\": grade\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during web search or document grading: {e}\")\n",
    "\n",
    "    return {\n",
    "        **state,  # Include all existing state\n",
    "        \"documents\": documents,\n",
    "        \"steps\": steps,\n",
    "        \"document_grades\": document_grades\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate test cases or perform a web search.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "    search_needed = state.get(\"search_needed\", False)\n",
    "    if search_needed:\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        return \"generate_test_cases\"\n",
    "\n",
    "def generate_test_cases(state):\n",
    "    vulnerabilities = state[\"analysis\"][\"vulnerabilities\"]\n",
    "    documents = state[\"documents\"]\n",
    "    steps = state[\"steps\"]\n",
    "    steps.append(\"generate_test_cases\")\n",
    "\n",
    "    result = rag_testcase_generator.invoke({\n",
    "        \"attack_tree_analysis\": json.dumps(state[\"analysis\"]),\n",
    "        \"context_documents\": \"\\n\".join([doc.page_content for doc in documents]),\n",
    "        \"attack_tree\": state[\"attack_tree\"],\n",
    "        \"format_instructions\": output_parser.get_format_instructions()\n",
    "    })\n",
    "\n",
    "    state[\"test_cases\"] = result.test_cases\n",
    "    return state\n",
    "\n",
    "def check_test_cases(state):\n",
    "    attack_tree = json.loads(state[\"attack_tree\"])\n",
    "    test_cases = state[\"test_cases\"]\n",
    "    steps = state[\"steps\"]\n",
    "    steps.append(\"check_test_cases\")\n",
    "\n",
    "    alignment_prompt = f\"\"\"\n",
    "    You are an expert security analyst with a critical eye for detail. Your task is to rigorously check if the generated test cases align with the attack tree, are complete, and are of high quality.\n",
    "\n",
    "    Attack Tree:\n",
    "    {json.dumps(attack_tree, indent=2)}\n",
    "\n",
    "    Generated Test Cases:\n",
    "    {json.dumps([tc.dict() for tc in test_cases], indent=2)}\n",
    "\n",
    "    Perform a thorough analysis of the test cases and provide the following:\n",
    "    1. Alignment: Are the test cases properly aligned with the vulnerabilities identified in the attack tree? Be extremely critical.\n",
    "    2. Completeness: Do the test cases cover ALL vulnerabilities mentioned in the attack tree? List any that are missing or inadequately covered.\n",
    "    3. Runnability: Is the code in the test cases runnable in Python? Are there any missing imports, setup steps, or other issues that would prevent immediate execution?\n",
    "    4. Quality: Assess the quality of each test case. Are they thorough? Do they actually test what they claim to test?\n",
    "    5. Improvements: Suggest specific, detailed improvements for each test case that falls short in any way.\n",
    "\n",
    "    Provide your analysis in a structured JSON format with the following keys:\n",
    "    - alignment_score (0-100, be very strict)\n",
    "    - completeness_score (0-100, be very strict)\n",
    "    - runnability_score (0-100, be very strict)\n",
    "    - quality_score (0-100, be very strict)\n",
    "    - missing_vulnerabilities (list of vulnerabilities not covered or inadequately covered)\n",
    "    - improvement_suggestions (list of objects, each containing:\n",
    "        - test_case_name: the name of the test case that needs improvement\n",
    "        - suggestions: list of specific, detailed suggestions for improving that test case)\n",
    "\n",
    "    Be extremely critical in your assessment. We need to ensure these test cases are of the highest possible quality.\n",
    "    \"\"\"\n",
    "\n",
    "    alignment_check = llm.invoke(alignment_prompt)\n",
    "\n",
    "    if isinstance(alignment_check, AIMessage):\n",
    "        alignment_check_content = alignment_check.content\n",
    "    else:\n",
    "        alignment_check_content = str(alignment_check)\n",
    "\n",
    "    try:\n",
    "        alignment_result = json.loads(alignment_check_content)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Warning: Could not parse JSON from LLM response. Using default values.\")\n",
    "        alignment_result = {\n",
    "            \"alignment_score\": 0,\n",
    "            \"completeness_score\": 0,\n",
    "            \"runnability_score\": 0,\n",
    "            \"quality_score\": 0,\n",
    "            \"missing_vulnerabilities\": [\"Could not determine\"],\n",
    "            \"improvement_suggestions\": [\n",
    "                {\n",
    "                    \"test_case_name\": \"Unknown\",\n",
    "                    \"suggestions\": [\"Regenerate all test cases due to parsing error\"]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    state[\"alignment_check\"] = alignment_result\n",
    "    state[\"steps\"] = steps\n",
    "    return state\n",
    "\n",
    "def regenerate_test_cases(state):\n",
    "    alignment_check = state[\"alignment_check\"]\n",
    "    steps = state[\"steps\"]\n",
    "    steps.append(\"regenerate_test_cases\")\n",
    "    regeneration_attempts = state.get(\"regeneration_attempts\", 0)\n",
    "    regeneration_attempts += 1\n",
    "    state[\"regeneration_attempts\"] = regeneration_attempts\n",
    "\n",
    "    if regeneration_attempts >= 10:\n",
    "        #print(\"Maximum regeneration attempts reached, ending workflow.\")\n",
    "        return state\n",
    "\n",
    "    missing_vulnerabilities = alignment_check.get(\"missing_vulnerabilities\", [])\n",
    "    improvement_suggestions = alignment_check.get(\"improvement_suggestions\", [])\n",
    "\n",
    "    # Map improvement suggestions to test cases\n",
    "    improvements_map = {improv[\"test_case_name\"]: improv[\"suggestions\"] for improv in improvement_suggestions}\n",
    "\n",
    "    # Extract test cases that need to be modified\n",
    "    test_cases_to_modify = []\n",
    "    for tc in state[\"test_cases\"]:\n",
    "        if tc.name in improvements_map:\n",
    "            test_cases_to_modify.append(tc)\n",
    "\n",
    "    # Prepare the prompt inputs\n",
    "    prompt_inputs = {\n",
    "        \"test_cases_to_modify\": json.dumps([tc.dict() for tc in test_cases_to_modify], indent=2),\n",
    "        \"improvements_map\": json.dumps(improvements_map, indent=2),\n",
    "        \"missing_vulnerabilities\": json.dumps(missing_vulnerabilities, indent=2),\n",
    "        \"format_instructions\": output_parser.get_format_instructions(),\n",
    "    }\n",
    "\n",
    "    # Regeneration prompt focusing on modifying specific test cases\n",
    "    regenerate_prompt = f\"\"\"\n",
    "    You are an elite security test engineer with extensive experience in creating comprehensive and robust test suites across various systems and infrastructures. Your critical task is to **modify specific test cases** based on the provided improvement suggestions, and **add new test cases** for any missing vulnerabilities.\n",
    "\n",
    "    ### Test Cases to Modify:\n",
    "    {prompt_inputs['test_cases_to_modify']}\n",
    "\n",
    "    ### Improvement Suggestions:\n",
    "    {prompt_inputs['improvements_map']}\n",
    "\n",
    "    ### Missing Vulnerabilities:\n",
    "    {prompt_inputs['missing_vulnerabilities']}\n",
    "\n",
    "    ### Instructions:\n",
    "    1. **Modify the test cases listed above** to incorporate all improvement suggestions specific to each test case. Only make changes where improvements are suggested; retain other content.\n",
    "    2. For each **missing vulnerability**, **create a new test case** that exactly addresses the vulnerability.\n",
    "    3. Ensure that all test cases use appropriate and actual code relevant to the system under test, utilizing standard libraries or APIs suitable for that system.\n",
    "    4. Include all necessary **setup**, including required imports and initialization of system components or services if needed.\n",
    "    5. The test code must be **complete, runnable Python code**. Do not use pseudocode or placeholders.\n",
    "    6. Follow **best practices** for the system or domain you are testing, and use appropriate methods and calls.\n",
    "    7. Each test case should demonstrate both the **vulnerable state and the secure state**.\n",
    "    8. Use **assert statements** to clearly indicate what constitutes a pass or fail condition.\n",
    "\n",
    "    {prompt_inputs['format_instructions']}\n",
    "    \"\"\"\n",
    "\n",
    "    # Invoke the LLM directly with the regenerate_prompt\n",
    "    llm_response = llm.invoke(regenerate_prompt)\n",
    "\n",
    "    # Extract the content from the AIMessage\n",
    "    if isinstance(llm_response, AIMessage):\n",
    "        llm_content = llm_response.content\n",
    "    else:\n",
    "        llm_content = str(llm_response)\n",
    "\n",
    "    # Parse the LLM output using the output parser\n",
    "    try:\n",
    "        parsed_output = output_parser.parse(llm_content)\n",
    "        modified_and_new_test_cases = parsed_output.test_cases\n",
    "        #print(f\"Successfully modified/added {len(modified_and_new_test_cases)} test cases.\")\n",
    "\n",
    "        # Create a mapping of test case names to test cases\n",
    "        existing_test_cases_map = {tc.id: tc for tc in state[\"test_cases\"]}\n",
    "\n",
    "        # Update the modified test cases in the existing test cases\n",
    "        for tc in modified_and_new_test_cases:\n",
    "            existing_test_cases_map[tc.name] = tc\n",
    "\n",
    "        # Update the state with the combined test cases\n",
    "        state[\"test_cases\"] = list(existing_test_cases_map.values())\n",
    "        assign_unique_test_case_numbers(state[\"test_cases\"])\n",
    "\n",
    "\n",
    "        # Verify that all missing vulnerabilities are addressed\n",
    "        addressed_vulnerabilities = set(tc.vulnerability_addressed for tc in state[\"test_cases\"])\n",
    "        still_missing = set(missing_vulnerabilities) - addressed_vulnerabilities\n",
    "\n",
    "        # if still_missing:\n",
    "        #     print(f\"Warning: The following vulnerabilities are still not addressed: {still_missing}\")\n",
    "        # else:\n",
    "        #     print(\"All previously missing vulnerabilities have been addressed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing LLM output: {e}\")\n",
    "        print(\"LLM Output:\", llm_content)\n",
    "        print(\"Regeneration failed due to parsing error.\")\n",
    "        # Optionally, set alignment scores to force termination\n",
    "        state[\"alignment_check\"] = {\n",
    "            \"alignment_score\": 0,\n",
    "            \"completeness_score\": 0,\n",
    "            \"runnability_score\": 0,\n",
    "            \"quality_score\": 0,\n",
    "            \"missing_vulnerabilities\": missing_vulnerabilities,\n",
    "            \"improvement_suggestions\": improvement_suggestions\n",
    "        }\n",
    "\n",
    "    state[\"steps\"] = steps\n",
    "    return state\n",
    "\n",
    "\n",
    "def decide_to_stop_or_regenerate(state):\n",
    "    alignment_check = state[\"alignment_check\"]\n",
    "    regeneration_attempts = state.get(\"regeneration_attempts\", 0)\n",
    "    #print(f\"Regeneration Attempts: {regeneration_attempts}\")\n",
    "\n",
    "    if not alignment_check.get(\"missing_vulnerabilities\") and not alignment_check.get(\"improvement_suggestions\") and (\n",
    "        alignment_check.get(\"alignment_score\", 0) > 90 and \n",
    "        alignment_check.get(\"completeness_score\", 0) > 90 and \n",
    "        alignment_check.get(\"runnability_score\", 0) > 90\n",
    "    ):\n",
    "        #print(\"All vulnerabilities addressed and alignment scores highly satisfactory, ending workflow.\")\n",
    "        return END\n",
    "    elif regeneration_attempts >= 10:\n",
    "        #print(\"Maximum regeneration attempts reached, ending workflow.\")\n",
    "        return END\n",
    "    else:\n",
    "        #print(\"Regenerating test cases.\")\n",
    "        return \"regenerate_test_cases\"\n",
    "\n",
    "# Define the workflow\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"analyze_attack_tree\", analyze_attack_tree)\n",
    "workflow.add_node(\"retrieve_documents\", retrieve_documents)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"web_search\", web_search)\n",
    "workflow.add_node(\"generate_test_cases\", generate_test_cases)\n",
    "workflow.add_node(\"check_test_cases\", check_test_cases)\n",
    "workflow.add_node(\"regenerate_test_cases\", regenerate_test_cases)\n",
    "\n",
    "# Build the graph\n",
    "workflow.add_edge(START, \"analyze_attack_tree\")\n",
    "workflow.add_edge(\"analyze_attack_tree\", \"retrieve_documents\")\n",
    "workflow.add_edge(\"retrieve_documents\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"generate_test_cases\": \"generate_test_cases\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"web_search\", \"generate_test_cases\")\n",
    "workflow.add_edge(\"generate_test_cases\", \"check_test_cases\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_test_cases\",\n",
    "    decide_to_stop_or_regenerate,\n",
    "    {\n",
    "        END: END,\n",
    "        \"regenerate_test_cases\": \"regenerate_test_cases\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"regenerate_test_cases\", \"check_test_cases\")\n",
    "\n",
    "custom_graph = workflow.compile()\n",
    "display(Image(custom_graph.get_graph(xray=True).draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_folder = './dataset/output_json/'\n",
    "output_folder = './LLama_output/'\n",
    "error_folder = './error_files/'  # Folder to store files that caused errors\n",
    "\n",
    "# Ensure output and error folders exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "os.makedirs(error_folder, exist_ok=True)\n",
    "\n",
    "# Get all JSON files in the input folder\n",
    "attack_tree_files = [f for f in os.listdir(input_folder) if f.endswith('.json')]\n",
    "\n",
    "# List to keep track of files that had errors\n",
    "error_files = []\n",
    "\n",
    "# Process each attack tree file\n",
    "for filename in tqdm(attack_tree_files, desc=\"Processing Files\"):\n",
    "    input_file_path = os.path.join(input_folder, filename)\n",
    "    output_file_name = os.path.splitext(filename)[0] + '_test_cases.json'\n",
    "    output_file_path = os.path.join(output_folder, output_file_name)\n",
    "\n",
    "    try:\n",
    "        # Load the attack tree\n",
    "        with open(input_file_path, 'r') as f:\n",
    "            attack_tree = json.load(f)\n",
    "\n",
    "        # Run the workflow\n",
    "        result = custom_graph.invoke(\n",
    "            {\n",
    "                \"attack_tree\": json.dumps(attack_tree),\n",
    "                \"steps\": []\n",
    "            },\n",
    "            config={\"recursion_limit\": 50}\n",
    "        )\n",
    "\n",
    "        # Get the test cases\n",
    "        test_cases = verify_test_cases(result.get('test_cases', []))\n",
    "\n",
    "        # Save test cases to the output file\n",
    "        with open(output_file_path, 'w') as f_out:\n",
    "            json.dump([tc.dict() for tc in test_cases], f_out, indent=2)\n",
    "\n",
    "        # Optionally, also save the alignment check results\n",
    "        alignment_check = result.get('alignment_check', {})\n",
    "        alignment_check_file_name = os.path.splitext(filename)[0] + '_alignment_check.json'\n",
    "        alignment_check_file_path = os.path.join(output_folder, alignment_check_file_name)\n",
    "        with open(alignment_check_file_path, 'w') as f_align:\n",
    "            json.dump(alignment_check, f_align, indent=2)\n",
    "\n",
    "        # Print status\n",
    "        print(f\"Processed {filename}, saved test cases to {output_file_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {filename}: {e}\")\n",
    "        error_files.append(filename)\n",
    "        # Move the problematic file to the error folder for later inspection\n",
    "        os.rename(input_file_path, os.path.join(error_folder, filename))\n",
    "        continue  # Continue to the next file\n",
    "\n",
    "# After processing all files, write the list of error files to a log\n",
    "error_log_path = os.path.join(output_folder, 'error_files.txt')\n",
    "with open(error_log_path, 'w') as f_error:\n",
    "    for error_file in error_files:\n",
    "        f_error.write(f\"{error_file}\\n\")\n",
    "\n",
    "print(f\"Processing complete. {len(error_files)} files had errors.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
